name: Continuous Monitoring & Feedback

on:
  # 1æ™‚é–“ã”ã¨ã«å®Ÿè¡Œ
  schedule:
    - cron: '0 * * * *'
  # Pushã‚¤ãƒ™ãƒ³ãƒˆå¾Œã‚‚å®Ÿè¡Œ
  push:
    branches: [main, master]
  workflow_dispatch:

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  monitor-app-metrics:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Monitoring
        run: |
          pip install prometheus-client psutil requests
          
      - name: Collect Metrics
        run: |
          cat > collect_metrics.py << 'EOF'
          import json
          import psutil
          import requests
          import time
          from datetime import datetime
          
          metrics = {
              "timestamp": datetime.now().isoformat(),
              "system": {},
              "application": {},
              "errors": [],
              "alerts": []
          }
          
          # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
          metrics["system"]["cpu_percent"] = psutil.cpu_percent(interval=1)
          metrics["system"]["memory_percent"] = psutil.virtual_memory().percent
          metrics["system"]["disk_usage"] = psutil.disk_usage('/').percent
          
          # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
          endpoints = [
              ("simple_api", "http://localhost:8001/health"),
              ("session_api", "http://localhost:8003/health"),
              ("web_ui", "http://localhost:5000/")
          ]
          
          for name, url in endpoints:
              try:
                  start = time.time()
                  response = requests.get(url, timeout=5)
                  elapsed = time.time() - start
                  
                  metrics["application"][name] = {
                      "status": response.status_code,
                      "response_time": elapsed,
                      "available": response.status_code == 200
                  }
                  
                  # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶
                  if elapsed > 3:
                      metrics["alerts"].append(f"{name} slow response: {elapsed:.2f}s")
                  if response.status_code != 200:
                      metrics["alerts"].append(f"{name} unhealthy: {response.status_code}")
                      
              except Exception as e:
                  metrics["application"][name] = {
                      "status": 0,
                      "error": str(e),
                      "available": False
                  }
                  metrics["alerts"].append(f"{name} is down: {str(e)}")
          
          # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°åˆ†æ
          try:
              with open("simple_api.log", "r") as f:
                  recent_logs = f.readlines()[-100:]
                  
              error_patterns = {
                  "rate_limit": "rate limit",
                  "timeout": "timeout",
                  "connection": "connection",
                  "authentication": "auth"
              }
              
              for pattern_name, pattern in error_patterns.items():
                  count = sum(1 for line in recent_logs if pattern in line.lower())
                  if count > 5:
                      metrics["errors"].append({
                          "type": pattern_name,
                          "count": count,
                          "severity": "high" if count > 10 else "medium"
                      })
          except:
              pass
          
          # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
          with open("metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          # å‰å›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨æ¯”è¼ƒ
          try:
              with open("previous_metrics.json", "r") as f:
                  previous = json.load(f)
              
              # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’æ¤œå‡º
              for endpoint in metrics["application"]:
                  if endpoint in previous.get("application", {}):
                      current_time = metrics["application"][endpoint].get("response_time", 0)
                      previous_time = previous["application"][endpoint].get("response_time", 0)
                      
                      if current_time > previous_time * 1.5:
                          metrics["alerts"].append(
                              f"Performance degradation in {endpoint}: {previous_time:.2f}s -> {current_time:.2f}s"
                          )
          except:
              pass
          
          # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
          with open("previous_metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          print(json.dumps(metrics, indent=2))
          EOF
          
          python collect_metrics.py
          
      - name: Analyze Trends
        id: analyze
        run: |
          cat > analyze_trends.py << 'EOF'
          import json
          import statistics
          
          with open("metrics.json", "r") as f:
              current = json.load(f)
          
          # ã‚¢ãƒ©ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã¯Issueã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹
          needs_attention = len(current.get("alerts", [])) > 0
          high_severity_errors = any(
              error.get("severity") == "high" 
              for error in current.get("errors", [])
          )
          
          print(f"::set-output name=needs_attention::{needs_attention or high_severity_errors}")
          print(f"::set-output name=metrics::{json.dumps(current)}")
          EOF
          
          python analyze_trends.py
          
      - name: Create Alert Issue
        if: steps.analyze.outputs.needs_attention == 'True'
        uses: actions/github-script@v7
        with:
          script: |
            const metrics = ${{ steps.analyze.outputs.metrics }};
            
            const issueBody = `## ğŸš¨ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆ
            
            **æ¤œå‡ºæ™‚åˆ»**: ${new Date().toLocaleString('ja-JP', { timeZone: 'Asia/Tokyo' })}
            
            ### ã‚¢ãƒ©ãƒ¼ãƒˆ
            ${metrics.alerts.map(alert => `- âš ï¸ ${alert}`).join('\n')}
            
            ### ã‚¨ãƒ©ãƒ¼å‚¾å‘
            ${metrics.errors.map(error => 
              `- ${error.type}: ${error.count}ä»¶ (é‡è¦åº¦: ${error.severity})`
            ).join('\n')}
            
            ### ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            - CPUä½¿ç”¨ç‡: ${metrics.system.cpu_percent}%
            - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: ${metrics.system.memory_percent}%
            - ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡: ${metrics.system.disk_usage}%
            
            ### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            ${Object.entries(metrics.application).map(([name, data]) => 
              `- ${name}: ${data.available ? 'âœ…' : 'âŒ'} (å¿œç­”æ™‚é–“: ${data.response_time?.toFixed(2)}s)`
            ).join('\n')}
            
            ---
            
            @claude ã“ã®ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆã‚’åˆ†æã—ã¦ã€ä»¥ä¸‹ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ï¼š
            
            1. æ ¹æœ¬åŸå› ã®åˆ†æ
            2. å³åº§ã®å¯¾å‡¦æ³•ã®ææ¡ˆ
            3. é•·æœŸçš„ãªæ”¹å–„ç­–ã®ææ¡ˆ
            4. å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£PRã®ä½œæˆ
            
            ç‰¹ã«ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ†æã—ã¦ãã ã•ã„ï¼š
            - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
            - ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŸå› 
            - ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©åŒ–
            - äºˆé˜²çš„å¯¾ç­–ã®å®Ÿè£…
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ğŸš¨ ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆ: ${metrics.alerts[0] || 'ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸æ¤œå‡º'}`,
              body: issueBody,
              labels: ['monitoring-alert', 'priority:high', 'ai-analysis']
            });
            
  user-feedback-collection:
    runs-on: ubuntu-latest
    
    steps:
      - name: Collect User Feedback
        run: |
          # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åé›†ï¼ˆå®Ÿéš›ã«ã¯ãƒ­ã‚°ã‚„DBã‹ã‚‰ï¼‰
          cat > feedback_analysis.py << 'EOF'
          import json
          import random
          
          # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
          feedback = {
              "response_times": [
                  random.uniform(0.5, 5.0) for _ in range(100)
              ],
              "error_reports": [
                  "ãƒ­ã‚°ã‚¤ãƒ³ã§ããªã„",
                  "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„",
                  "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒåˆ†ã‹ã‚Šã«ãã„"
              ],
              "feature_requests": [
                  "ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã®æ”¹å–„",
                  "æ¤œç´¢æ©Ÿèƒ½ã®å¼·åŒ–",
                  "ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œã®æ”¹å–„"
              ],
              "satisfaction_score": random.uniform(3.0, 5.0)
          }
          
          # åˆ†æ
          avg_response_time = sum(feedback["response_times"]) / len(feedback["response_times"])
          slow_requests = sum(1 for t in feedback["response_times"] if t > 3.0)
          
          analysis = {
              "avg_response_time": avg_response_time,
              "slow_request_percentage": (slow_requests / len(feedback["response_times"])) * 100,
              "top_issues": feedback["error_reports"][:3],
              "satisfaction_score": feedback["satisfaction_score"],
              "improvement_areas": []
          }
          
          if avg_response_time > 2.0:
              analysis["improvement_areas"].append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–")
          
          if analysis["satisfaction_score"] < 4.0:
              analysis["improvement_areas"].append("ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£æ”¹å–„")
          
          with open("feedback_analysis.json", "w") as f:
              json.dump(analysis, f, indent=2)
          
          EOF
          
          python feedback_analysis.py
          
      - name: Upload Analytics
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-data
          path: |
            metrics.json
            feedback_analysis.json